<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Nguyen (Will) Nguyen</title>

  <meta name="author" content="Jon Barron">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/buffalo_icon.png">
</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Nguyen (Will) Nguyen</name>
                  </p>
                  <p>Currently, I am working at University of Rochester advised by Professor <a
                      href="https://www.cs.rochester.edu/~cxu22/">Chenliang Xu</a>, where I work on instructional video
                    understanding and vision-language problems.
                  </p>
                  <p>
                    I had nearly 3 wonderful years at <a href="https://www.vinai.io/">VinAI Research</a> where I worked
                    on Scene text spotting problem under the supervision of Professor <a
                      href="https://www3.cs.stonybrook.edu/~minhhoai/">Nguyen Minh Hoai</a>. I did my bachelor at <a
                      href="https://e.uet.vnu.edu.vn/">University of Engineering and Technology - VNU</a>, where I had
                    chances to work with Professor <a href="https://sites.google.com/site/xiemhoang/">Hoang Van
                      Xiem</a>.
                  </p>
                  <p style="text-align:center">
                    <a href="mailto:nguyennm1024@gmail.com">Email</a> &nbsp/&nbsp
                    <a href="data/CV_Nguyen-2.pdf">CV</a> &nbsp/&nbsp
                    <a href="data/Nguyen-bio.txt">Bio</a> &nbsp/&nbsp
                    <a href="https://scholar.google.com/citations?user=kYok1lsAAAAJ&hl=en">Google Scholar</a>
                    &nbsp/&nbsp
                    <a href="https://twitter.com/nguyennm1024">Twitter</a> &nbsp/&nbsp
                    <a href="https://github.com/nguyennm1024">Github</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images/ManhNguyen.png"><img style="width:70%;max-width:70%" alt="profile photo"
                      src="images/ManhNguyen_circle.png" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>

          <div style="width:100%; margin-right:auto; margin-left:auto;">
            <div style="padding:20px; vertical-align:middle;">
              <heading>News</heading>
              <div style="height:200px; overflow:auto;">
                <ul>
                  <li>03/2024: One paper accepted at NAACL 2024.</li> <br>
                  <li>03/2024: I will be joining Bosch Research US as a research intern in the summer of 2024.</li> <br>
                  <li>07/2023: One paper accepted at AV4D Workshop, ICCV 2023.</li> <br>
                  <li>08/2022: I started my journey with the University of Rochester since the Fall 2022.</li>  <br>
                  <li>01/2022: I began working as an AI Research Engineer with the applied team at VinAI Research.</li> <br>
                  <li>03/2021: One paper accepted at CVPR 2021.</li> <br>
                  <li>07/2020: I graduated with Distinction from Vietnam National University in 2020.</li> <br>
                  <li>12/2019: I started my journey with VinAI Research as an AI Research Resident in December 2019.</li> <br>
                </ul>
              </div>
            </div>
          </div>
          

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Research</heading>
                  <p>
                    I'm interested in computer vision, natural language processing, and machine learning (especially
                    deep learning). Much of my research lies in optical character recognition, focusing on scene text
                    recognition. I am also fond of Vision-Language problems, such as understanding visual content using natural
                    language.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

       
          <table
          style="width:100%; border:0px; border-spacing:0px; border-collapse:separate; margin-right:auto; margin-left:auto;">
          <tbody>
            
            <td style="padding:20px; width:25%; vertical-align:middle;">
              <img src='images/ea.png' width="160">
      
            </td>

            <td style="padding:20px; width:75%; vertical-align:middle;">
              <a href="">
                <papertitle>Anonymous</papertitle>
              </a>
              <br>
              <em>Under review</em>, 2024
              <br>
              <a href="">github</a> /
              <a href="">paper</a>
              <p style="line-height:1.5;">
                A single multimodal LLM designed for comprehensively solving all tasks related to egocentric video understanding.
              </p>
            </td>
            </tr>
          </tbody>
        </table>

          <table
          style="width:100%; border:0px; border-spacing:0px; border-collapse:separate; margin-right:auto; margin-left:auto;">
          <tbody>
            
            <td style="padding:20px; width:25%; vertical-align:middle;">
              <img src='images/oscar.png' width="160">
      
            </td>

            <td style="padding:20px; width:75%; vertical-align:middle;">
              <a href="">
                <papertitle>OSCaR: Object State Captioning and State Change Representation</papertitle>
              </a>
              <br>
              <strong>Nguyen Nguyen</strong>,
              <a href="https://jing-bi.github.io">Jing Bi</a>,
              <a href="https://alivosoughi.com">Ali Vosoughi</a>,
              <a href="https://www.yapengtian.com">Yapeng Tian</a>,
              <a href="https://pooyanfazli.com">Pooyan Fazli</a>,
              <a href="https://www.cs.rochester.edu/~cxu22/">Chenliang Xu</a>
              <br>
              <em>NAACL (Findings)</em>, 2024
              <br>
              <a href="https://github.com/nguyennm1024/OSCaR">github</a> /
              <a href="https://arxiv.org/abs/2402.17128">paper</a>
              <p style="line-height:1.5;">
                A new task of understanding the state of objects and the progression of object state changes. Besides, we trained a Multimodal-LLM that significantly surpasses previous state-of-the-art models. Our model achieved 90% quality of GPT4-V on both GPT4 and human evaluations.
              </p>
            </td>
            </tr>
          </tbody>
        </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>


              <!-- <tr onmouseout="npil_stop()" onmouseover="npil_start()"> -->
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">

                  <img src='images/linguistics_detection.png' width="160">

                  <script type="text/javascript">
                    function npil_start() {
                      document.getElementById('npil_image').style.opacity = "1";
                    }

                    function npil_stop() {
                      document.getElementById('npil_image').style.opacity = "0";
                    }
                    npil_stop()
                  </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="">
                  <papertitle>Efficiently Leveraging Linguistics Knowledge for Scene Text Spotting</papertitle>
                </a>
                <br>

                <strong>Nguyen Nguyen</strong>,
                <a href="https://www.yapengtian.com">Yapeng Tian</a>,
                <a href="https://www.cs.rochester.edu/~cxu22/">Chenliang Xu</a>
                <br>
                <em>Under review</em>
                <br>
                
              <a href="https://arxiv.org/abs/2402.17134">paper</a>
                <p></p>
                <p>
                  A simple but effective approach to incorporate language knowledge from large text corpus for
                  improving both text detection and recognition.
                </p>
              </td>
      </tr>


    </tbody>
  </table>

  <table
    style="width:100%; border:0px; border-spacing:0px; border-collapse:separate; margin-right:auto; margin-left:auto;">
    <tbody>
      <td style="padding:20px; width:25%; vertical-align:middle;">
        <img src='images/misar.jpeg' width="160">

      </td>
      <td style="padding:20px; width:75%; vertical-align:middle;">
        <a href="">
          <papertitle>MISAR: A Multimodal Instructional System with Augmented Reality</papertitle>
        </a>
        <br>
        <a href="https://jing-bi.github.io">Jing Bi</a>*,
        <strong>Nguyen Nguyen*</strong>,
        <a href="https://alivosoughi.com">Ali Vosoughi</a>*,
        <a href="https://www.cs.rochester.edu/~cxu22/">Chenliang Xu</a> (* equal contribution)
        <br>
        <em>AV4D Workshop, ICCV</em>, 2023
        <br>
        <a href="https://github.com/nguyennm1024/misar">github</a> /
        <a href="https://av4d.org/papers/iccv23/p12.pdf">paper</a>
        <p style="line-height:1.5;">
          A comprehensive system designed to guide humans to work more efficiently and accurately. Our system leverages
          the power of LLMs to interpret and process information from visual, auditory, and contextual dimensions.
        </p>
      </td>
      </tr>
    </tbody>
  </table>


  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>


      <tr onmouseout="npil_stop()" onmouseover="npil_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='npil_image'>
              <img src='images/st_after.png' width="160">
            </div>
            <img src='images/st_before.png' width="160">
          </div>
          <script type="text/javascript">
            function npil_start() {
              document.getElementById('npil_image').style.opacity = "1";
            }

            function npil_stop() {
              document.getElementById('npil_image').style.opacity = "0";
            }
            npil_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://www.vinai.io/publication-posts/dictionary-guided-scene-text-recognition">
            <papertitle>Dictionary-guided Scene Text Recognition</papertitle>
          </a>
          <br>

          <strong>Nguyen Nguyen</strong>,
          <a href="">Thu Nguyen</a>,
          <a href="https://www3.cs.stonybrook.edu/~tquangvinh/">Vinh Tran</a>,
          <a href="https://www.fit.hcmus.edu.vn/~tmtriet/">Minh Triet Tran</a>,
          <a href="https://scholar.google.com/citations?user=I8bNZakAAAAJ&hl=en">Thanh Duc Ngo</a>,
          <a href="https://ix.cs.uoregon.edu/~thien/">Thien Huu Nguyen</a>,
          <a href="https://www3.cs.stonybrook.edu/~minhhoai/">Minh Hoai Nguyen</a>
          <br>
          <em>CVPR</em>, 2021
          <br>
          <a href="https://www.vinai.io/publication-posts/dictionary-guided-scene-text-recognition">project
            page</a> /
          <a href="https://github.com/VinAIResearch/dict-guided">github</a> /
          <a
            href="https://openaccess.thecvf.com/content/CVPR2021/html/Nguyen_Dictionary-Guided_Scene_Text_Recognition_CVPR_2021_paper.html">paper</a>
          <p></p>
          <p>
            A novel approach to incorporate dictionary on both training and testing phases. Additionally, we
            also introduced a novel Vietnamese scene text dataset (VinText), the largest scene text dataset for
            Vietnamese.
          </p>
        </td>
      </tr>


    </tbody>
  </table>



  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody>
      <tr>
        <td>
          <heading>Services</heading>
        </td>
      </tr>
    </tbody>
  </table>
  <table width="100%" align="center" border="0" cellpadding="20">
    <tbody>
      <tr>
        <td style="padding:20px;width:5%;vertical-align:middle"><img src="images/reviewer.jpg"></td>
        <td width="75%" valign="center">
          <p><b>Reviewer</b>
            <br>
            <i>WACV 2022, CVPR 2023, CVPR 2024, ACMMM 2024</i>
          </p>
        </td>
      </tr>
      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/ai-challenge.png"></td>
        <td width="75%" valign="center">
          <p><b>Competition jury member (2021)</b>
            <br>
            <i>Ho Chi Minh city AI Challenge 2021: Vietnamese Scene Text Recognition</i>
          </p>
        </td>
      </tr>
      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="images/teaching-assistant.png" alt="TA">
        </td>
        <td width="75%" valign="center">
          <p><b>Teaching assistant (2018 - 2019)</b>
            <br>
            <i>University of Engineering and Technology - Vietnam National University</i>
            <br>
            Teaching assistant in several computer vision and machine learning courses for Samsung Display
            Vietnam’s staff
          </p>
        </td>
      </tr>
    </tbody>
  </table>
  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr>
        <td style="padding:0px">
          <br>
          <p>This website uses source code from <a href="http://jonbarron.info">http://jonbarron.info</a> </p>
        </td>
      </tr>
    </tbody>
  </table>
  </td>
  </tr>
  </table>
</body>

</html>
